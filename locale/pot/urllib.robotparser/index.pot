# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Doug Hellmann
# This file is distributed under the same license as the PyMOTW-3 package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PyMOTW-3 \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2017-04-15 15:56-0300\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/urllib.robotparser/index.rst:3
msgid "urllib.robotparser --- Internet Spider Access Control"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:8
msgid "Parse ``robots.txt`` file used to control Internet spiders"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:10
msgid ":mod:`robotparser` implements a parser for the ``robots.txt`` file format, including a function that checks if a given user agent can access a resource.  It is intended for use in well-behaved spiders, or other crawler applications that need to either be throttled or otherwise restricted."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:17
#: ../../source/urllib.robotparser/index.rst:0
msgid "robots.txt"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:19
msgid "The ``robots.txt`` file format is a simple text-based access control system for computer programs that automatically access web resources (\"spiders\", \"crawlers\", etc.).  The file is made up of records that specify the user agent identifier for the program followed by a list of URLs (or URL prefixes) the agent may not access."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:25
msgid "This is the ``robots.txt`` file for ``https://pymotw.com/``:"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:30
msgid "It prevents access to some of the parts of the site that are expensive to compute and would overload the server if a search engine tried to index them.  For a more complete set of examples of ``robots.txt``, refer to `The Web Robots Page`_."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:36
msgid "Testing Access Permissions"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:38
msgid "Using the data presented earlier, a simple crawler can test whether it is allowed to download a page using ``RobotFileParser.can_fetch()``."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:0
msgid "urllib_robotparser_simple.py"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:45
msgid "The URL argument to ``can_fetch()`` can be a path relative to the root of the site, or full URL."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:72
msgid "Long-lived Spiders"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:74
msgid "An application that takes a long time to process the resources it downloads or that is throttled to pause between downloads should check for new ``robots.txt`` files periodically based on the age of the content it has downloaded already.  The age is not managed automatically, but there are convenience methods to make tracking it easier."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:0
msgid "urllib_robotparser_longlived.py"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:85
msgid "This extreme example downloads a new ``robots.txt`` file if the one it has is more than one second old."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:111
msgid "A nicer version of the long-lived application might request the modification time for the file before downloading the entire thing. On the other hand, ``robots.txt`` files are usually fairly small, so it is not that much more expensive to just retrieve the entire document again."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:120
msgid ":pydoc:`urllib.robotparser`"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:122
msgid "`The Web Robots Page`_ -- Description of ``robots.txt`` format."
msgstr ""

